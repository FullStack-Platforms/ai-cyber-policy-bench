# Configuration file for AI Cyber Policy Benchmark
# Copy this file and rename to config.cfg with your actual values

[OpenAI]
# Optional. Set these if you are testing OpenAI models.
api_key = 

# Optional. Set this if you are using an alternative OpenAI compatible endpoint, like ollama running locally
#openai_compatible_url = http://localhost:8080/v1/
#openai_compatible_url = https://api.together.xyz

[OpenRouter]
# Optional. Set this if you are using OpenRouter for model access
api_key =

[Huggingface]
# Optional. This allows the script to download gated models.
access_token =

# Optional. This is where models will be downloaded to.
# - defaults to ~/.cache/huggingface/hub
cache_dir = 

[Vector Database]
# Vector Database Settings for document storage and retrieval
db_path = ./vector_db
embedding_model = all-MiniLM-L6-v2
batch_size = 100
vector_space = cosine
default_search_results = 5

[Results upload]
# Optional. Set this to allow uploading of results to a google sheets spreadsheet.
# Note: this feature requires extra configuration (see README).
google_spreadsheet_url =

[Cyber Policy Benchmark]
# judge_model_api: openai / mistralai / anthropic / openrouter
judge_model_api = openrouter
judge_model = gpt-5-chat
judge_model_api_key = 


# Experiment Settings
output_dir = ./experiment_results
cache_dir = ./experiment_cache

[Models]
# Default evaluation models (comma-separated)
default_eval_models = gpt-5-chat,mistralai/mistral-nemo,qwen/qwen3-30b-a3b,qwen/qwen3-coder,z-ai/glm-4.5,gpt-5-mini,moonshotai/kimi-k2,google/gemini-2.5-flash,deepseek/deepseek-v3,google/gemini-2.5-pro,anthropic/claude-sonnet-4,x-ai/grok-4,anthropic/claude-3.7-sonnet,anthropic/claude-opus-4.1

# Default judge models (comma-separated)
default_judge_models = moonshotai/kimi-k2,gpt-5-chat,anthropic/claude-sonnet-4,qwen/qwen3-coder

[Scoring]
# Dual Judge Configuration
judge_mode = dual  # single/dual
judge_model_1 = moonshotai/kimi-k2
judge_model_2 = gpt-4.1
judge_weight_1 = 0.5
judge_weight_2 = 0.5
score_threshold_alert = 0.3  # Alert if score below threshold

[Evaluation]
# Evaluation Pipeline Settings
max_retries = 3
retry_delay = 2
timeout_seconds = 30
parallel_requests = 5
max_response_tokens = 1000
vector_context_results = 5

[CI/CD]
# CI/CD Pipeline Settings
run_integration_tests = true
benchmark_on_pr = false
minimum_coverage = 80

[Options]
# Set to true or false
trust_remote_code = true

[Benchmarks to run]
# Define benchmarks in the following format:
# run_id, instruction_template, model_path, lora_path, quantization, n_iterations, inference_engine, ooba_params, downloader_filters

# Details:
#
# - run_id: A name to identify the benchmark run
# - instruction_template: The filename of the instruction template defining the prompt format, minus the .yaml (e.g. Alpaca)
# - model_path: Huggingface model ID, local path, or OpenAI model name
# - lora_path (optional): Path to local lora adapter
# - quantization: Using bitsandbytes package (8bit, 4bit, None)
# - n_iterations: Number of benchmark iterations (final score will be an average)
# - inference_engine: Set this to transformers, openai, ooba or llama.cpp.
# - ooba_params (optional): Any additional ooba params for loading this model (overrides the global setting above)
# - downloader_filters (optional): Specify --include or --exclude patterns (using same syntax as huggingface-cli download)

# Examples:
#
# myrun1, openai_api, gpt-3.5-turbo, , , 1, openai, ,
# myrun2, Llama-v2, meta-llama/Llama-2-7b-chat-hf, /path/to/local/lora/adapter, 8bit, 3, transformers, , ,
# myrun3, Alpaca, ~/my_local_model, , None, 1, ooba, --loader transformers --n_ctx 1024 --n-gpu-layers -1, 
# myrun4, Mistral, TheBloke/Mistral-7B-Instruct-v0.2-GGUF, , None, 1, ooba, --loader llama.cpp --n-gpu-layers -1 --tensor_split 1,3,5,7, --include ["*Q3_K_M.gguf", "*.json"]
# myrun5, Mistral, mistralai/Mistral-7B-Instruct-v0.2, , None, 1, ooba, --loader transformers --gpu-memory 12, --exclude "*.bin"
# myrun6, ChatML, model_name, , None, 1, llama.cpp, None,