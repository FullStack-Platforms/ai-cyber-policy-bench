#!/usr/bin/env python3
"""
Test script for validating the enhanced cyber policy evaluation system.
Tests the critical fixes for empty response handling and qualitative scoring.
"""

import asyncio
import json
from src.scorer import AccuracyScorer, ScoringMethod

async def test_empty_response_handling():
    """Test that empty and invalid responses are correctly scored as 0.0"""
    scorer = AccuracyScorer()
    
    print("Testing Empty Response Handling...")
    print("=" * 60)
    
    test_question = "What SOC 2 controls are required for access management?"
    ideal_answer = "SOC 2 requires CC6.1 (logical access controls), CC6.2 (access authorization), and CC6.3 (access removal)."
    
    # Test cases for invalid responses
    invalid_responses = [
        "",  # Empty
        "   ",  # Whitespace only
        "Error: Model timeout",  # Error message
        "MODEL_FAILURE: Connection failed",  # Model failure
        "I don't know",  # Non-substantive
        "N/A",  # Non-substantive
        "Sorry, I can't help with that",  # Non-substantive
        "Hi!",  # Too short
    ]
    
    for i, response in enumerate(invalid_responses):
        print(f"\n{i+1}. Testing response: '{response}'")
        
        # Test validation
        is_valid, reason = scorer.validate_response(response)
        print(f"   Validation: {'PASS' if not is_valid else 'FAIL'} - {reason}")
        
        # Test control reference scoring
        control_result = scorer.control_reference_score(response, ideal_answer)
        print(f"   Control Reference Score: {control_result.accuracy_score:.3f}")
        
        # Test LLM judge scoring
        judge_result = await scorer.llm_judge_score(test_question, response, ideal_answer)
        print(f"   LLM Judge Score: {judge_result.accuracy_score:.3f}")
        
        # Test contextual relevance
        relevance_result = scorer.contextual_relevance_score(response, test_question, ideal_answer)
        print(f"   Contextual Relevance Score: {relevance_result.accuracy_score:.3f}")
        
        # Verify all scores are 0.0 for invalid responses
        if (control_result.accuracy_score == 0.0 and 
            judge_result.accuracy_score == 0.0 and 
            relevance_result.accuracy_score == 0.0):
            print("   âœ“ PASS: All scores correctly 0.0 for invalid response")
        else:
            print("   âœ— FAIL: Some scores not 0.0 for invalid response")

async def test_control_reference_edge_cases():
    """Test the fixed control reference scoring logic"""
    scorer = AccuracyScorer()
    
    print("\n\nTesting Control Reference Edge Cases...")
    print("=" * 60)
    
    # Test case 1: Both ideal and model have no controls, but model response is empty
    print("\n1. Empty response vs ideal with no controls:")
    empty_response = ""
    no_control_ideal = "Ensure proper documentation of security procedures."
    
    result = scorer.control_reference_score(empty_response, no_control_ideal)
    print(f"   Score: {result.accuracy_score:.3f} (should be 0.0)")
    print(f"   Explanation: {result.explanation}")
    
    # Test case 2: Both ideal and model have no controls, but model response is substantive
    print("\n2. Substantive response vs ideal with no controls:")
    good_response = "Security documentation should be comprehensive, regularly updated, and accessible to authorized personnel. It should include incident response procedures, access control policies, and compliance requirements to ensure organizational security posture."
    
    result = scorer.control_reference_score(good_response, no_control_ideal)
    print(f"   Score: {result.accuracy_score:.3f} (should be 1.0)")
    print(f"   Explanation: {result.explanation}")
    
    # Test case 3: Empty response vs ideal with controls
    print("\n3. Empty response vs ideal with controls:")
    control_ideal = "Implement CC6.1 for logical access controls and CC6.2 for access authorization."
    
    result = scorer.control_reference_score(empty_response, control_ideal)
    print(f"   Score: {result.accuracy_score:.3f} (should be 0.0)")
    print(f"   Explanation: {result.explanation}")

async def test_new_scoring_methods():
    """Test the new contextual relevance and completeness scoring methods"""
    scorer = AccuracyScorer()
    
    print("\n\nTesting New Scoring Methods...")
    print("=" * 60)
    
    question = "What are the key GDPR requirements for data breach notification?"
    ideal_answer = "GDPR Article 33 requires breach notification to supervisory authorities within 72 hours, and Article 34 requires notification to data subjects when high risk exists."
    
    # Test good response
    good_response = """GDPR establishes strict data breach notification requirements:

1. **Authority Notification (Article 33)**: Organizations must notify the relevant supervisory authority within 72 hours of becoming aware of a breach, unless the breach is unlikely to result in risk to rights and freedoms.

2. **Individual Notification (Article 34)**: Data subjects must be notified without undue delay when the breach is likely to result in high risk to their rights and freedoms.

3. **Documentation Requirements**: All breaches must be documented regardless of notification requirements, including facts, effects, and remedial actions taken."""
    
    # Test poor response
    poor_response = "You need to tell people about data breaches quickly."
    
    print("\n1. Testing Contextual Relevance Scoring:")
    print("\nGood Response:")
    relevance_result = scorer.contextual_relevance_score(good_response, question, ideal_answer)
    print(f"   Score: {relevance_result.accuracy_score:.3f}")
    print(f"   Details: {relevance_result.explanation}")
    
    print("\nPoor Response:")
    relevance_result = scorer.contextual_relevance_score(poor_response, question, ideal_answer)
    print(f"   Score: {relevance_result.accuracy_score:.3f}")
    print(f"   Details: {relevance_result.explanation}")
    
    print("\n2. Testing Completeness Scoring:")
    print("\nGood Response:")
    completeness_result = scorer.completeness_score(good_response, question, ideal_answer)
    print(f"   Score: {completeness_result.accuracy_score:.3f}")
    print(f"   Details: {completeness_result.explanation}")
    
    print("\nPoor Response:")
    completeness_result = scorer.completeness_score(poor_response, question, ideal_answer)
    print(f"   Score: {completeness_result.accuracy_score:.3f}")
    print(f"   Details: {completeness_result.explanation}")

async def test_composite_scoring():
    """Test the composite policy scoring method."""
    scorer = AccuracyScorer()
    
    # Sample policy question (from cyber_eval_041)
    question = """A SaaS startup wants to draft an acceptable usage policy that is compliant with SOC 2 and CMMC level 1. Create the policy using the short_prompt.md format with the following inputs: Org overview (SaaS startup, 50 employees, cloud-based services), Compliance (SOC 2 Type II, CMMC Level 1), Constraints (remote work environment, BYOD policy)."""
    
    # Sample good response (well-structured policy)
    good_response = """# Acceptable Usage Policy â€“ SaaS Startup

## Purpose & Scope
This policy defines acceptable use of company systems and data to maintain SOC 2 and CMMC Level 1 compliance. Applies to all employees and contractors.

## Policy Statements
1. **System Access**: Multi-factor authentication required for all systems
2. **Data Handling**: Confidential data must be encrypted in transit and at rest
3. **Usage Restrictions**: No unauthorized software installation or credential sharing
4. **Device Management**: BYOD devices must be enrolled in MDM
5. **Monitoring**: All system usage is monitored and logged

## Roles & Responsibilities
- **IT Security**: Policy enforcement and monitoring
- **Employees**: Compliance with usage requirements
- **Management**: Policy exception approvals

## Compliance & Enforcement
Violations result in progressive discipline up to termination.

## Review Cycle
Annual review by IT Security team.

## Framework Mappings
| Policy Section | SOC 2 Control | CMMC Control | Description |
|---|---|---|---|
| System Access | CC6.1, CC6.2 | AC.L1-3.1.1 | Access controls |
| Data Handling | CC6.7 | SC.L1-3.13.1 | Data protection |
"""
    
    # Sample poor response (missing structure)
    poor_response = """Users should use strong passwords and not share them. Data should be protected. Management will enforce this policy. We follow SOC 2 and CMMC requirements."""
    
    ideal_answer = "A structured policy with all required sections per short_prompt.md format."
    
    print("Testing Composite Policy Scoring...")
    print("=" * 50)
    
    # Test good response
    print("\n1. Testing well-structured policy response:")
    result = await scorer.composite_policy_score(question, good_response, ideal_answer)
    print(f"Score: {result.accuracy_score:.3f}")
    print(f"Explanation: {result.explanation}")
    
    # Test poor response
    print("\n2. Testing poorly structured policy response:")
    result = await scorer.composite_policy_score(question, poor_response, ideal_answer)
    print(f"Score: {result.accuracy_score:.3f}")
    print(f"Explanation: {result.explanation}")
    
    print("\n" + "=" * 50)
    print("Testing individual scoring methods...")
    
    # Test structural validation
    struct_result = scorer.structural_validation_score(good_response, question)
    print(f"\nStructural validation score: {struct_result.accuracy_score:.3f}")
    print(f"Details: {struct_result.explanation}")
    
    # Test conciseness scoring
    concise_result = scorer.conciseness_score(good_response, question)
    print(f"\nConciseness score: {concise_result.accuracy_score:.3f}")
    print(f"Details: {concise_result.explanation}")

def test_question_loading():
    """Test loading and filtering of evaluation questions."""
    print("\nTesting question loading and filtering...")
    print("=" * 50)
    
    # Load questions with short_prompt format
    with open('data/prompts/cyber_evals.jsonl', 'r') as f:
        questions = []
        for line in f:
            line = line.strip()
            if line:
                question = json.loads(line)
                questions.append(question)
    
    # Filter for policy generation questions
    policy_questions = [q for q in questions if q['metadata'].get('format') == 'short_prompt']
    
    print(f"Total questions: {len(questions)}")
    print(f"Policy generation questions: {len(policy_questions)}")
    
    print("\nPolicy questions by framework:")
    for q in policy_questions:
        frameworks = q['metadata'].get('framework', 'N/A')
        industry = q['metadata'].get('industry', 'N/A')
        difficulty = q['metadata'].get('difficulty', 'N/A')
        print(f"  {q['question_id']}: {frameworks} ({industry}, {difficulty})")

if __name__ == "__main__":
    print("ðŸ§ª CYBER POLICY EVALUATION SYSTEM - COMPREHENSIVE TEST SUITE")
    print("=" * 80)
    print("Testing critical fixes for empty response handling and enhanced scoring\n")
    
    # Run all tests
    asyncio.run(test_empty_response_handling())
    asyncio.run(test_control_reference_edge_cases())
    asyncio.run(test_new_scoring_methods())
    test_question_loading()
    asyncio.run(test_composite_scoring())
    
    print("\n" + "=" * 80)
    print("ðŸŽ‰ COMPREHENSIVE TESTING COMPLETE!")
    print("\nKey fixes validated:")
    print("âœ“ Empty responses now correctly score 0.0 (prevents masking failures)")
    print("âœ“ Error messages properly detected and scored as failures") 
    print("âœ“ Control reference logic fixed for edge cases")
    print("âœ“ Enhanced rubrics capture qualitative nuance")
    print("âœ“ Response validation prevents scoring of invalid responses")
    print("âœ“ Composite scoring includes completeness and relevance assessment")