# Configuration file for AI Cyber Policy Benchmark
# Copy this file and rename to config.cfg with your actual values

# =============================================================================
# API CREDENTIALS
# Configure access to model providers
# =============================================================================

[OpenAI]
# OpenAI API credentials
api_key = 
# Optional: Alternative OpenAI-compatible endpoint (e.g., local inference servers)
openai_compatible_url = 

[OpenRouter]
# OpenRouter API for accessing multiple models
api_key = 

[Anthropic]
# Anthropic API credentials
api_key = 

[Huggingface]
# Hugging Face Hub access
access_token = 
cache_dir = 

# =============================================================================
# MODEL CONFIGURATION
# Define which models to use for evaluation and scoring
# =============================================================================

[Models]
# Evaluation models (comma-separated list)
eval_models = gpt-5-chat,z-ai/glm-4.5,mistralai/mistral-nemo,moonshotai/kimi-k2,qwen/qwen3-30b-a3b,qwen/qwen3-coder,gpt-5-mini,google/gemini-2.5-flash,deepseek/deepseek-chat-v3-0324,google/gemini-2.5-pro,anthropic/claude-sonnet-4,x-ai/grok-4,anthropic/claude-3.7-sonnet,anthropic/claude-opus-4.1

# Judge models for scoring (comma-separated list)
judge_models = moonshotai/kimi-k2,gpt-5-chat,anthropic/claude-sonnet-4,qwen/qwen3-coder

# Model discovery settings
use_dynamic_models = false
model_cache_hours = 24
max_eval_models = 20
max_judge_models = 4

# =============================================================================
# SCORING CONFIGURATION
# Configure how responses are scored and judged
# =============================================================================

[Scoring]
# Scoring method: single, dual, or ensemble
scoring_method = dual

# Primary judge model
primary_judge = moonshotai/kimi-k2
primary_judge_weight = 0.5

# Secondary judge model (for dual/ensemble scoring)
secondary_judge = gpt-5-chat
secondary_judge_weight = 0.5

# Scoring thresholds and alerts
score_threshold_alert = 0.3
min_consensus_threshold = 0.7
max_score_deviation = 0.3

# Scoring parameters
max_scoring_retries = 3
scoring_timeout = 45
include_reasoning = true

# =============================================================================
# EVALUATION PIPELINE
# Control evaluation execution and behavior
# =============================================================================

[Evaluation]
# Request handling
max_retries = 3
retry_delay = 2.0
timeout_seconds = 30
parallel_requests = 5

# Response parameters
max_response_tokens = 1000
temperature = 0.7
top_p = 1.0

# Context and retrieval
vector_context_results = 5
max_context_length = 8000
include_source_attribution = true

# Evaluation modes (set to false to disable specific modes)
enable_no_context = true
enable_raw_files = true
enable_vector_db = true

# =============================================================================
# VECTOR DATABASE
# Document storage and retrieval configuration
# =============================================================================

[VectorDatabase]
# Storage location
db_path = ./vector_db

# Embedding configuration
embedding_model = all-MiniLM-L6-v2
embedding_dimensions = 384
batch_size = 100

# Search and retrieval
vector_space = cosine
default_search_results = 5
similarity_threshold = 0.7

# Performance settings
chunk_size = 512
chunk_overlap = 50
max_chunks_per_document = 1000

# =============================================================================
# SYSTEM PATHS AND STORAGE
# Configure where data is stored and processed
# =============================================================================

[Paths]
# Core directories
output_dir = ./experiment_results
cache_dir = ./experiment_cache
data_dir = ./data
frameworks_dir = ./data/cyber-frameworks

# Specific files
prompts_file = ./data/prompts/cyber_evals.jsonl
chunks_dir = ./output/chunks
logs_dir = ./logs

# =============================================================================
# REPORTING AND EXPORT
# Configure output formats and destinations
# =============================================================================

[Reporting]
# Report formats (comma-separated: json, csv, html, markdown)
output_formats = json,csv,html

# Report details
include_detailed_results = true
include_model_metadata = true
include_timing_info = true
include_error_analysis = true

# External integrations
google_spreadsheet_url = 
slack_webhook_url = 

# =============================================================================
# SYSTEM OPTIONS
# General system behavior and feature flags
# =============================================================================

[Options]
# Security and trust
trust_remote_code = true
validate_configs = true
strict_mode = false

# Logging and debugging
log_level = INFO
enable_debug_mode = false
log_api_requests = false
log_responses = false

# Performance
enable_caching = true
cache_responses = true
parallel_processing = true

# =============================================================================
# CI/CD AND TESTING
# Automated testing and integration settings
# =============================================================================

[Testing]
# Test execution
run_integration_tests = true
benchmark_on_pr = false
minimum_coverage = 80

# Test parameters
test_model_limit = 2
test_question_limit = 3
test_timeout = 120

# Quality gates
min_accuracy_threshold = 0.6
max_error_rate = 0.1
performance_regression_threshold = 0.05