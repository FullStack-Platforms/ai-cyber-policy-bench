# Cyber Policy Benchmark

[![CI Pipeline](https://github.com/your-org/cyber-policy-bench/actions/workflows/ci.yml/badge.svg)](https://github.com/your-org/cyber-policy-bench/actions/workflows/ci.yml)
[![Benchmark Tests](https://github.com/your-org/cyber-policy-bench/actions/workflows/benchmark-test.yml/badge.svg)](https://github.com/your-org/cyber-policy-bench/actions/workflows/benchmark-test.yml)

A comprehensive benchmarking suite for evaluating AI models on cybersecurity policy and compliance questions. This tool evaluates model performance across different contexts including no context, raw framework documents, and vector database retrieval.

## üöÄ Features

### Core Functionality
- **Multi-Context Evaluation**: Tests models with no context, raw framework files, and vector database retrieval
- **Dual Judge Scoring**: Advanced scoring system with two independent LLM judges for reliability
- **Comprehensive Framework Support**: Covers SOC 2, PCI DSS, HIPAA, GDPR, NIST, CMMC, and more
- **Vector Database Integration**: Smart context retrieval using ChromaDB with multi-collection framework support

### Advanced Capabilities
- **Graceful Failure Handling**: Robust error handling with fallback mechanisms
- **Performance Monitoring**: Detailed metrics tracking and performance analysis
- **HTML Reporting**: Beautiful, comprehensive reports with visualizations
- **CI/CD Integration**: Automated testing and validation pipelines
- **Model Management**: Intelligent model discovery and capability detection

## üìã Table of Contents

1. [Quick Start](#-quick-start)
2. [Installation](#-installation)
3. [Configuration](#-configuration)
4. [Usage](#-usage)
5. [Architecture](#-architecture)
6. [Development](#-development)
7. [CI/CD](#-cicd)
8. [Contributing](#-contributing)

## üöÄ Quick Start

### Prerequisites
- Python 3.9+
- API keys for OpenRouter and/or OpenAI
- Git

### Basic Setup
```bash
# Clone the repository
git clone https://github.com/your-org/cyber-policy-bench.git
cd cyber-policy-bench

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure the benchmark
cp config.example.cfg config.cfg
# Edit config.cfg with your API keys

# Run a quick test
python cyber_policy_bench.py --models 2 --questions 3 --setup-db
```

## üì¶ Installation

### Standard Installation
```bash
pip install -r requirements.txt
```

### Development Installation
```bash
# Install with development dependencies
pip install -r requirements.txt
pip install black

# Install pre-commit hooks (optional)
pre-commit install
```

### Dependencies
- **Core**: `openai`, `chromadb`, `requests`, `pathlib`
- **Document Processing**: `docling` (for PDF/document processing)
- **Development**: `black`

## ‚öôÔ∏è Configuration

The benchmark uses a comprehensive configuration system with support for multiple API providers and advanced settings.

### Basic Configuration (`config.cfg`)

```ini
[OpenRouter]
api_key = your-openrouter-key

[OpenAI]  
api_key = your-openai-key
# openai_compatible_url = http://localhost:8080/v1/  # For local models

[Scoring]
# Dual Judge Configuration
judge_mode = dual  # single/dual
judge_model_1 = anthropic/claude-sonnet-4
judge_model_2 = openai/gpt-4o
judge_weight_1 = 0.5
judge_weight_2 = 0.5
score_threshold_alert = 0.3

[Evaluation]
max_retries = 3
retry_delay = 2
timeout_seconds = 30
parallel_requests = 5

[Vector Database]
db_path = ./vector_db

[Cyber Policy Benchmark]
output_dir = ./experiment_results
cache_dir = ./experiment_cache
```

### Environment Variables
```bash
# Override config values with environment variables
export OPENROUTER_API_KEY="your-key"
export OPENAI_API_KEY="your-key"
export BENCHMARK_OUTPUT_DIR="./results"
```

## üéØ Usage

### Basic Evaluation
```bash
# Run with default settings (2 models, 3 questions)
python cyber_policy_bench.py --setup-db

# Custom evaluation
python cyber_policy_bench.py --models 5 --questions 10 --setup-db

# Use existing vector database
python cyber_policy_bench.py --models 3 --questions 5
```

### Command Line Options
- `--models N`: Number of models to evaluate (default: 2)
- `--questions N`: Number of questions to test (default: 3)
- `--setup-db`: Setup/rebuild vector database from framework chunks

### Programmatic Usage
```python
import asyncio
from src.evaluator import CyberPolicyEvaluator, EvaluationMode
from src.scorer import TwoJudgeScorer
from src.reporter import create_benchmark_reporter

async def run_custom_evaluation():
    # Initialize components
    evaluator = CyberPolicyEvaluator()
    scorer = TwoJudgeScorer()
    reporter = create_benchmark_reporter()
    
    # Load questions
    questions = evaluator.load_evaluation_questions()[:5]
    models = ["openai/gpt-4o", "anthropic/claude-sonnet-4"]
    modes = [EvaluationMode.NO_CONTEXT, EvaluationMode.VECTOR_DB]
    
    # Run evaluation
    results = await evaluator.run_evaluation(models, questions, modes)
    scored_results = await scorer.score_evaluation_results(results)
    
    # Generate reports
    reports = reporter.generate_all_reports(scored_results)
    print(f"Reports generated: {reports}")

# Run the evaluation
asyncio.run(run_custom_evaluation())
```

## üèóÔ∏è Architecture

### Core Components

```
src/
‚îú‚îÄ‚îÄ base.py          # Base classes and mixins
‚îú‚îÄ‚îÄ utils.py         # Common utilities and helpers
‚îú‚îÄ‚îÄ models.py        # Model management and discovery
‚îú‚îÄ‚îÄ evaluator.py     # Core evaluation engine
‚îú‚îÄ‚îÄ scorer.py        # Scoring system with dual judges
‚îú‚îÄ‚îÄ reporter.py      # Comprehensive reporting system
‚îú‚îÄ‚îÄ vectorize.py     # Document processing and vectorization
‚îî‚îÄ‚îÄ db.py           # Vector database operations
```

### Key Features

#### Dual Judge Scoring System
The benchmark uses an advanced dual judge system for reliable scoring:
- **Parallel Evaluation**: Both judges evaluate simultaneously
- **Graceful Fallback**: If one judge fails, uses the other
- **Discrepancy Detection**: Alerts when judges disagree significantly
- **Weighted Averaging**: Configurable weights for different judges

#### Smart Context Retrieval
- **Framework Detection**: Automatically detects relevant frameworks in questions
- **Multi-Collection Search**: Searches specific framework collections when detected
- **Fallback Search**: Falls back to global search if no frameworks detected
- **Context Truncation**: Prevents context window overflow

#### Robust Error Handling
- **Retry Logic**: Exponential backoff for API failures
- **Input Validation**: Comprehensive input validation and sanitization
- **Graceful Degradation**: System continues working even with partial failures
- **Detailed Logging**: Comprehensive logging for debugging

## üß™ Development

### Code Quality
```bash
# Linting
ruff check src/

# Formatting
black src/

# Type checking
mypy src/ --ignore-missing-imports
```

### Project Structure
```
cyber-policy-bench/
‚îú‚îÄ‚îÄ src/                    # Source code
‚îú‚îÄ‚îÄ data/                   # Framework documents and evaluation data
‚îú‚îÄ‚îÄ .github/workflows/      # CI/CD pipelines
‚îú‚îÄ‚îÄ experiment_results/     # Benchmark results
‚îú‚îÄ‚îÄ reports/               # Generated reports
‚îú‚îÄ‚îÄ vector_db/             # Vector database storage
‚îî‚îÄ‚îÄ config.cfg            # Configuration file
```

## üöÄ CI/CD

### GitHub Actions Workflows

#### CI Pipeline (`.github/workflows/ci.yml`)
- **Multi-Python Testing**: Tests on Python 3.9, 3.10, 3.11
- **Code Quality**: Linting, formatting, and type checking
- **Security Scanning**: Automated security analysis with bandit
- **Coverage Reporting**: Codecov integration

#### Benchmark Testing (`.github/workflows/benchmark-test.yml`)
- **Mini Benchmarks**: Quick validation with 2 models, 3 questions
- **Scoring Validation**: Ensures all results receive scores
- **Mode Coverage**: Validates all evaluation modes work
- **Reproducibility**: Tests consistency between runs

#### Scheduled Benchmarks (`.github/workflows/scheduled-benchmark.yml`)
- **Weekly Full Runs**: Comprehensive benchmarks every Sunday
- **Performance Tracking**: Long-term performance monitoring
- **Regression Detection**: Automatic alerts for performance drops
- **Visualization**: Generates charts and analysis

### Configuration for CI

Set these secrets in your GitHub repository:
```bash
OPENROUTER_API_KEY=your-openrouter-key
OPENAI_API_KEY=your-openai-key  
```

## üìä Reports and Monitoring

### HTML Reports
The benchmark generates comprehensive HTML reports with:
- **Overall Performance Metrics**: Success rates, score distributions
- **Model Comparisons**: Side-by-side performance analysis
- **Evaluation Mode Analysis**: Performance by context type
- **Detailed Statistics**: Standard deviations, confidence intervals
- **Visual Progress Bars**: Interactive performance visualizations

### JSON Exports
Machine-readable exports for:
- **Integration**: Easy integration with other tools
- **Analysis**: Custom analysis and visualization
- **Archival**: Long-term performance tracking

### Judge Performance Tracking
Monitor dual judge system performance:
- **Success Rates**: Individual judge reliability
- **Fallback Usage**: How often fallbacks are needed
- **Discrepancy Detection**: Judge agreement analysis

## ü§ù Contributing

### Development Workflow
1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature-name`
3. **Implement** your changes with tests
4. **Run** the test suite: `pytest`
5. **Check** code quality: `ruff check src/` and `black src/`
6. **Submit** a pull request

### Code Standards
- **PEP 8** compliance (enforced by black)
- **Type hints** for public APIs
- **Comprehensive tests** for new features
- **Documentation** for public functions
- **Error handling** with appropriate exceptions

### Adding New Features

#### New Evaluation Modes
1. Add mode to `EvaluationMode` enum in `evaluator.py`
2. Implement context retrieval logic
3. Update `evaluate_single_question` method
4. Add tests in `tests/test_evaluator.py`

#### New Scoring Methods
1. Add method to `ScoringMethod` enum in `scorer.py`
2. Implement scoring logic in `AccuracyScorer` or `TwoJudgeScorer`
3. Update `score_response` method
4. Add tests in `tests/test_scorer.py`

#### New Frameworks
1. Add framework documents to `data/cyber-frameworks/`
2. Create `metadata.toml` file
3. Update framework detection in `evaluator.py`
4. Add test questions to `data/prompts/cyber_evals.jsonl`

## üìö API Reference

### Core Classes

#### `CyberPolicyEvaluator`
Main evaluation engine with support for multiple context modes.

```python
evaluator = CyberPolicyEvaluator(vector_db=db)
results = await evaluator.run_evaluation(models, questions, modes)
```

#### `TwoJudgeScorer`
Advanced scoring system with dual judge support.

```python
scorer = TwoJudgeScorer(
    judge_model_1="anthropic/claude-sonnet-4",
    judge_model_2="openai/gpt-4o"
)
scored_results = await scorer.score_evaluation_results(results)
```

#### `BenchmarkReporter`
Comprehensive reporting and visualization system.

```python
reporter = create_benchmark_reporter()
report_paths = reporter.generate_all_reports(results)
```

### Configuration

#### Dual Judge Settings
```ini
[Scoring]
judge_mode = dual
judge_model_1 = anthropic/claude-sonnet-4  
judge_model_2 = openai/gpt-4o
judge_weight_1 = 0.6  # Primary judge weight
judge_weight_2 = 0.4  # Secondary judge weight
```

#### Performance Tuning
```ini  
[Evaluation]
max_retries = 3           # API retry attempts
retry_delay = 2           # Base delay between retries
timeout_seconds = 30      # Request timeout
parallel_requests = 5     # Concurrent requests
```

## üîß Troubleshooting

### Common Issues

#### "No API keys found"
- Ensure `config.cfg` exists and has valid API keys
- Check environment variables: `OPENROUTER_API_KEY` or `OPENAI_API_KEY`
- Verify key format and permissions

#### "No scores generated"
- Check judge model availability and API limits  
- Verify context window limits aren't exceeded
- Review dual judge fallback statistics
- Enable debug logging for detailed error information

#### "Vector database not found"
- Run with `--setup-db` flag to initialize database
- Check `db_path` in configuration
- Ensure framework documents exist in `data/cyber-frameworks/`

#### Performance Issues
- Reduce `parallel_requests` in configuration
- Increase `timeout_seconds` for slow models
- Use model filtering to test fewer models
- Check API rate limits

### Debug Mode
Enable detailed logging:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

Or set environment variable:
```bash
export PYTHONPATH=. LOGLEVEL=DEBUG python cyber_policy_bench.py
```

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- [OpenRouter](https://openrouter.ai/) for model access
- [ChromaDB](https://www.trychroma.com/) for vector database functionality
- [Docling](https://github.com/DS4SD/docling) for document processing
- The cybersecurity community for framework documentation
- [EQ-Bench](https://github.com/EQ-bench/EQ-Bench) for the initial inspiration, 

## üìû Support

- **Issues**: [GitHub Issues](https://github.com/your-org/cyber-policy-bench/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-org/cyber-policy-bench/discussions)
- **Documentation**: [Wiki](https://github.com/your-org/cyber-policy-bench/wiki)

---

**Built with ‚ù§Ô∏è for the cybersecurity community**